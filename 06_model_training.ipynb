{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Linear Regression, Random Forest, XGBoost\n",
    "\n",
    "This notebook implements three machine learning models for battery capacity prediction:\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "The models are trained on the processed battery dataset and evaluated using battery-based train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "Load the final processed datasets from the previous notebooks. We'll use the battery-based splits to ensure proper evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets not found. Running fallback data preparation...\n",
      "Fallback failed: cannot import name 'load_and_prepare_ml_data' from 'data_processing_utils' (/Users/ben/Documents/Stanford/TECH 27/finalproject/data_processing_utils.py)\n",
      "Please run notebooks 02-05 first to generate processed data.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_and_prepare_ml_data' from 'data_processing_utils' (/Users/ben/Documents/Stanford/TECH 27/finalproject/data_processing_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Try to load from 05_final_processing output\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     X_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_data/X_train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     X_val = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mprocessed_data/X_val.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'processed_data/X_train.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Fallback: Load and process data from scratch\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_processing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_and_prepare_ml_data\n\u001b[32m     27\u001b[39m     X_train, X_val, X_test, y_train, y_val, y_test = load_and_prepare_ml_data()\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrepared datasets using fallback method:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_and_prepare_ml_data' from 'data_processing_utils' (/Users/ben/Documents/Stanford/TECH 27/finalproject/data_processing_utils.py)"
     ]
    }
   ],
   "source": [
    "# Load processed data from previous notebooks\n",
    "try:\n",
    "    # Try to load from 05_final_processing output\n",
    "    X_train = pd.read_csv('processed_data/X_train.csv')\n",
    "    X_val = pd.read_csv('processed_data/X_val.csv')\n",
    "    X_test = pd.read_csv('processed_data/X_test.csv')\n",
    "    y_train = pd.read_csv('processed_data/y_train.csv')\n",
    "    y_val = pd.read_csv('processed_data/y_val.csv')\n",
    "    y_test = pd.read_csv('processed_data/y_test.csv')\n",
    "    \n",
    "    # Convert to numpy arrays and flatten y\n",
    "    y_train = y_train.values.ravel()\n",
    "    y_val = y_val.values.ravel()\n",
    "    y_test = y_test.values.ravel()\n",
    "    \n",
    "    print(f\"Loaded processed datasets:\")\n",
    "    print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Processed datasets not found. Running fallback data preparation...\")\n",
    "    \n",
    "    # Fallback: Load and process data from scratch\n",
    "    try:\n",
    "        from data_processing_utils import load_and_prepare_ml_data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = load_and_prepare_ml_data()\n",
    "        print(f\"Prepared datasets using fallback method:\")\n",
    "        print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "        print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fallback failed: {e}\")\n",
    "        print(\"Please run notebooks 02-05 first to generate processed data.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics about the target variable\n",
    "print(\"Target variable (capacity) statistics:\")\n",
    "print(f\"Training set - Mean: {np.mean(y_train):.3f}, Std: {np.std(y_train):.3f}\")\n",
    "print(f\"Validation set - Mean: {np.mean(y_val):.3f}, Std: {np.std(y_val):.3f}\")\n",
    "print(f\"Test set - Mean: {np.mean(y_test):.3f}, Std: {np.std(y_test):.3f}\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].hist(y_train, bins=30, alpha=0.7, color='blue')\n",
    "axes[0].set_title('Training Set Capacity Distribution')\n",
    "axes[0].set_xlabel('Capacity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(y_val, bins=30, alpha=0.7, color='orange')\n",
    "axes[1].set_title('Validation Set Capacity Distribution')\n",
    "axes[1].set_xlabel('Capacity')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].hist(y_test, bins=30, alpha=0.7, color='green')\n",
    "axes[2].set_title('Test Set Capacity Distribution')\n",
    "axes[2].set_xlabel('Capacity')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation Functions\n",
    "\n",
    "Define utility functions for model training, prediction, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MSE: {mse:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    print(f\"R²: {r2:.6f}\")\n",
    "    \n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "def plot_predictions(y_true, y_pred, model_name, dataset_name=''):\n",
    "    \"\"\"Plot predicted vs actual values.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(min(y_true), min(y_pred))\n",
    "    max_val = max(max(y_true), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel('Actual Capacity')\n",
    "    plt.ylabel('Predicted Capacity')\n",
    "    plt.title(f'{model_name} - Predicted vs Actual {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R² score to plot\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    plt.text(0.05, 0.95, f'R² = {r2:.4f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, model_name, dataset_name=''):\n",
    "    \"\"\"Plot residuals to check for patterns.\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[0].scatter(y_pred, residuals, alpha=0.6)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Predicted Capacity')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title(f'{model_name} - Residuals vs Predicted {dataset_name}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals histogram\n",
    "    axes[1].hist(residuals, bins=30, alpha=0.7)\n",
    "    axes[1].set_xlabel('Residuals')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'{model_name} - Residuals Distribution {dataset_name}')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Linear Regression model...\")\n",
    "\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_val_pred = lr_model.predict(X_val)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "lr_train_metrics = evaluate_model(y_train, lr_train_pred, \"Linear Regression (Training)\")\n",
    "lr_val_metrics = evaluate_model(y_val, lr_val_pred, \"Linear Regression (Validation)\")\n",
    "lr_test_metrics = evaluate_model(y_test, lr_test_pred, \"Linear Regression (Test)\")\n",
    "\n",
    "print(\"\\nLinear Regression training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Linear Regression results\n",
    "plot_predictions(y_val, lr_val_pred, \"Linear Regression\", \"(Validation Set)\")\n",
    "plot_residuals(y_val, lr_val_pred, \"Linear Regression\", \"(Validation Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "# Train Random Forest with default parameters first\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "rf_train_metrics = evaluate_model(y_train, rf_train_pred, \"Random Forest (Training)\")\n",
    "rf_val_metrics = evaluate_model(y_val, rf_val_pred, \"Random Forest (Validation)\")\n",
    "rf_test_metrics = evaluate_model(y_test, rf_test_pred, \"Random Forest (Test)\")\n",
    "\n",
    "print(\"\\nRandom Forest training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])],\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 most important features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest results\n",
    "plot_predictions(y_val, rf_val_pred, \"Random Forest\", \"(Validation Set)\")\n",
    "plot_residuals(y_val, rf_val_pred, \"Random Forest\", \"(Validation Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "# Train XGBoost with default parameters first\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "xgb_train_metrics = evaluate_model(y_train, xgb_train_pred, \"XGBoost (Training)\")\n",
    "xgb_val_metrics = evaluate_model(y_val, xgb_val_pred, \"XGBoost (Validation)\")\n",
    "xgb_test_metrics = evaluate_model(y_test, xgb_test_pred, \"XGBoost (Test)\")\n",
    "\n",
    "print(\"\\nXGBoost training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])],\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 most important features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_xgb_features = xgb_importance.head(20)\n",
    "plt.barh(range(len(top_xgb_features)), top_xgb_features['importance'])\n",
    "plt.yticks(range(len(top_xgb_features)), top_xgb_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 most important features (XGBoost):\")\n",
    "print(xgb_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XGBoost results\n",
    "plot_predictions(y_val, xgb_val_pred, \"XGBoost\", \"(Validation Set)\")\n",
    "plot_residuals(y_val, xgb_val_pred, \"XGBoost\", \"(Validation Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Training_R2': [lr_train_metrics['r2'], rf_train_metrics['r2'], xgb_train_metrics['r2']],\n",
    "    'Training_RMSE': [lr_train_metrics['rmse'], rf_train_metrics['rmse'], xgb_train_metrics['rmse']],\n",
    "    'Validation_R2': [lr_val_metrics['r2'], rf_val_metrics['r2'], xgb_val_metrics['r2']],\n",
    "    'Validation_RMSE': [lr_val_metrics['rmse'], rf_val_metrics['rmse'], xgb_val_metrics['rmse']],\n",
    "    'Test_R2': [lr_test_metrics['r2'], rf_test_metrics['r2'], xgb_test_metrics['r2']],\n",
    "    'Test_RMSE': [lr_test_metrics['rmse'], rf_test_metrics['rmse'], xgb_test_metrics['rmse']]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# R² comparison\n",
    "metrics = ['Training_R2', 'Validation_R2', 'Test_R2']\n",
    "x_pos = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0, 0].bar(x_pos + i*width, results_df[metric], width, \n",
    "                   label=metric.replace('_', ' '), alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].set_title('R² Score Comparison')\n",
    "axes[0, 0].set_xticks(x_pos + width)\n",
    "axes[0, 0].set_xticklabels(results_df['Model'])\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_metrics = ['Training_RMSE', 'Validation_RMSE', 'Test_RMSE']\n",
    "for i, metric in enumerate(rmse_metrics):\n",
    "    axes[0, 1].bar(x_pos + i*width, results_df[metric], width, \n",
    "                   label=metric.replace('_', ' '), alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_title('RMSE Comparison')\n",
    "axes[0, 1].set_xticks(x_pos + width)\n",
    "axes[0, 1].set_xticklabels(results_df['Model'])\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction comparison on validation set\n",
    "axes[1, 0].scatter(y_val, lr_val_pred, alpha=0.6, label='Linear Regression', s=20)\n",
    "axes[1, 0].scatter(y_val, rf_val_pred, alpha=0.6, label='Random Forest', s=20)\n",
    "axes[1, 0].scatter(y_val, xgb_val_pred, alpha=0.6, label='XGBoost', s=20)\n",
    "\n",
    "min_val = min(y_val.min(), lr_val_pred.min(), rf_val_pred.min(), xgb_val_pred.min())\n",
    "max_val = max(y_val.max(), lr_val_pred.max(), rf_val_pred.max(), xgb_val_pred.max())\n",
    "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "axes[1, 0].set_xlabel('Actual Capacity')\n",
    "axes[1, 0].set_ylabel('Predicted Capacity')\n",
    "axes[1, 0].set_title('All Models - Validation Set Predictions')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals comparison\n",
    "lr_residuals = y_val - lr_val_pred\n",
    "rf_residuals = y_val - rf_val_pred\n",
    "xgb_residuals = y_val - xgb_val_pred\n",
    "\n",
    "axes[1, 1].hist(lr_residuals, alpha=0.6, label='Linear Regression', bins=30)\n",
    "axes[1, 1].hist(rf_residuals, alpha=0.6, label='Random Forest', bins=30)\n",
    "axes[1, 1].hist(xgb_residuals, alpha=0.6, label='XGBoost', bins=30)\n",
    "axes[1, 1].set_xlabel('Residuals')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Residuals Distribution Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the best performing model on the test set for final performance assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model based on validation R²\n",
    "best_model_idx = results_df['Validation_R2'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "best_val_r2 = results_df.loc[best_model_idx, 'Validation_R2']\n",
    "best_test_r2 = results_df.loc[best_model_idx, 'Test_R2']\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Validation R²: {best_val_r2:.6f}\")\n",
    "print(f\"Test R²: {best_test_r2:.6f}\")\n",
    "\n",
    "# Get predictions from best model\n",
    "if best_model_name == 'Linear Regression':\n",
    "    best_test_pred = lr_test_pred\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_test_pred = rf_test_pred\n",
    "else:  # XGBoost\n",
    "    best_test_pred = xgb_test_pred\n",
    "\n",
    "# Final test set evaluation\n",
    "print(f\"\\nFinal Test Set Evaluation - {best_model_name}:\")\n",
    "final_metrics = evaluate_model(y_test, best_test_pred, best_model_name)\n",
    "\n",
    "# Plot final results\n",
    "plot_predictions(y_test, best_test_pred, best_model_name, \"(Test Set)\")\n",
    "plot_residuals(y_test, best_test_pred, best_model_name, \"(Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Summary and Recommendations\n",
    "\n",
    "Summary of model performance and recommendations for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModels Trained:\")\n",
    "print(\"1. Linear Regression - Simple baseline model\")\n",
    "print(\"2. Random Forest - Ensemble method with feature importance\")\n",
    "print(\"3. XGBoost - Gradient boosting with regularization\")\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "print(\"\\nPerformance Summary (Test Set R²):\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"{row['Model']}: {row['Test_R2']:.6f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test Set R²: {best_test_r2:.6f}\")\n",
    "print(f\"Test Set RMSE: {results_df.loc[best_model_idx, 'Test_RMSE']:.6f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    train_r2 = row['Training_R2']\n",
    "    val_r2 = row['Validation_R2']\n",
    "    gap = train_r2 - val_r2\n",
    "    print(f\"{row['Model']}: Training R² = {train_r2:.4f}, Validation R² = {val_r2:.4f}, Gap = {gap:.4f}\")\n",
    "    \n",
    "    if gap > 0.1:\n",
    "        print(f\"  WARNING: {row['Model']} shows signs of overfitting (gap > 0.1)\")\n",
    "    elif gap > 0.05:\n",
    "        print(f\"  CAUTION: {row['Model']} shows mild overfitting (gap > 0.05)\")\n",
    "    else:\n",
    "        print(f\"  GOOD: {row['Model']} shows good generalization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Deploy {best_model_name} for production use\")\n",
    "print(\"2. Monitor model performance on new data\")\n",
    "print(\"3. Consider hyperparameter tuning for further improvements\")\n",
    "print(\"4. Implement feature selection to reduce model complexity\")\n",
    "print(\"5. Consider ensemble methods combining multiple models\")\n",
    "\n",
    "if best_model_name != 'Linear Regression':\n",
    "    print(f\"6. Use feature importance from {best_model_name} for feature engineering\")\n",
    "\n",
    "print(\"\\nModel training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
