{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Linear Regression, Random Forest, XGBoost\n",
    "\n",
    "This notebook implements three machine learning models for battery capacity prediction:\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "The models are trained on the processed battery dataset and evaluated using battery-based train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "Load the final processed datasets from the previous notebooks. We'll use the battery-based splits to ensure proper evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load processed data from notebook 05_final_processing.ipynb\nimport pickle\nfrom pathlib import Path\n\n# Define output directory\nOUTPUT_DIR = Path('processed_data')\n\n# Load the final ML datasets from notebook 5\nml_datasets_path = OUTPUT_DIR / 'final_ml_ml_datasets.pkl'\n\nif ml_datasets_path.exists():\n    print(f\"Loading processed datasets from {ml_datasets_path}...\")\n    with open(ml_datasets_path, 'rb') as f:\n        ml_datasets = pickle.load(f)\n    \n    # Extract the datasets\n    X_train = ml_datasets['X_train']\n    X_val = ml_datasets['X_val']\n    X_test = ml_datasets['X_test']\n    y_train = ml_datasets['y_train']\n    y_val = ml_datasets['y_val']\n    y_test = ml_datasets['y_test']\n    \n    # Get feature names if available\n    feature_cols = ml_datasets.get('feature_cols', \n                                   [f'feature_{i}' for i in range(X_train.shape[1])])\n    \n    print(f\"✅ Successfully loaded datasets from notebook 5\")\n    print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n    print(f\"Validation set: {X_val.shape[0]} samples\")\n    print(f\"Test set: {X_test.shape[0]} samples\")\n    print(f\"Features available: {len(feature_cols)}\")\n    \nelse:\n    print(f\"❌ Error: Could not find {ml_datasets_path}\")\n    print(\"Please run notebook 05_final_processing.ipynb first to generate the datasets.\")\n    raise FileNotFoundError(f\"Dataset file not found: {ml_datasets_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics about the target variable\n",
    "print(\"Target variable (capacity) statistics:\")\n",
    "print(f\"Training set - Mean: {np.mean(y_train):.3f}, Std: {np.std(y_train):.3f}\")\n",
    "print(f\"Validation set - Mean: {np.mean(y_val):.3f}, Std: {np.std(y_val):.3f}\")\n",
    "print(f\"Test set - Mean: {np.mean(y_test):.3f}, Std: {np.std(y_test):.3f}\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].hist(y_train, bins=30, alpha=0.7, color='blue')\n",
    "axes[0].set_title('Training Set Capacity Distribution')\n",
    "axes[0].set_xlabel('Capacity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(y_val, bins=30, alpha=0.7, color='orange')\n",
    "axes[1].set_title('Validation Set Capacity Distribution')\n",
    "axes[1].set_xlabel('Capacity')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].hist(y_test, bins=30, alpha=0.7, color='green')\n",
    "axes[2].set_title('Test Set Capacity Distribution')\n",
    "axes[2].set_xlabel('Capacity')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation Functions\n",
    "\n",
    "Define utility functions for model training, prediction, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MSE: {mse:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    print(f\"R²: {r2:.6f}\")\n",
    "    \n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "def plot_predictions(y_true, y_pred, model_name, dataset_name=''):\n",
    "    \"\"\"Plot predicted vs actual values.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(min(y_true), min(y_pred))\n",
    "    max_val = max(max(y_true), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel('Actual Capacity')\n",
    "    plt.ylabel('Predicted Capacity')\n",
    "    plt.title(f'{model_name} - Predicted vs Actual {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R² score to plot\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    plt.text(0.05, 0.95, f'R² = {r2:.4f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, model_name, dataset_name=''):\n",
    "    \"\"\"Plot residuals to check for patterns.\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[0].scatter(y_pred, residuals, alpha=0.6)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Predicted Capacity')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title(f'{model_name} - Residuals vs Predicted {dataset_name}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals histogram\n",
    "    axes[1].hist(residuals, bins=30, alpha=0.7)\n",
    "    axes[1].set_xlabel('Residuals')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'{model_name} - Residuals Distribution {dataset_name}')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Linear Regression model...\")\n",
    "\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_val_pred = lr_model.predict(X_val)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "lr_train_metrics = evaluate_model(y_train, lr_train_pred, \"Linear Regression (Training)\")\n",
    "lr_val_metrics = evaluate_model(y_val, lr_val_pred, \"Linear Regression (Validation)\")\n",
    "lr_test_metrics = evaluate_model(y_test, lr_test_pred, \"Linear Regression (Test)\")\n",
    "\n",
    "print(\"\\nLinear Regression training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Linear Regression results\n",
    "plot_predictions(y_val, lr_val_pred, \"Linear Regression\", \"(Validation Set)\")\n",
    "plot_residuals(y_val, lr_val_pred, \"Linear Regression\", \"(Validation Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "# Train Random Forest with default parameters first\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "rf_train_metrics = evaluate_model(y_train, rf_train_pred, \"Random Forest (Training)\")\n",
    "rf_val_metrics = evaluate_model(y_val, rf_val_pred, \"Random Forest (Validation)\")\n",
    "rf_test_metrics = evaluate_model(y_test, rf_test_pred, \"Random Forest (Test)\")\n",
    "\n",
    "print(\"\\nRandom Forest training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature importance analysis\nfeature_importance = pd.DataFrame({\n    'feature': feature_cols,  # Use the feature_cols loaded from the pickle file\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Plot top 20 most important features\nplt.figure(figsize=(10, 8))\ntop_features = feature_importance.head(20)\nplt.barh(range(len(top_features)), top_features['importance'])\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Feature Importance')\nplt.title('Top 20 Most Important Features (Random Forest)')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTop 10 most important features:\")\nprint(feature_importance.head(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest results\n",
    "plot_predictions(y_val, rf_val_pred, \"Random Forest\", \"(Validation Set)\")\n",
    "plot_residuals(y_val, rf_val_pred, \"Random Forest\", \"(Validation Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "# Train XGBoost with default parameters first\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "xgb_train_metrics = evaluate_model(y_train, xgb_train_pred, \"XGBoost (Training)\")\n",
    "xgb_val_metrics = evaluate_model(y_val, xgb_val_pred, \"XGBoost (Validation)\")\n",
    "xgb_test_metrics = evaluate_model(y_test, xgb_test_pred, \"XGBoost (Test)\")\n",
    "\n",
    "print(\"\\nXGBoost training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# XGBoost feature importance\nxgb_importance = pd.DataFrame({\n    'feature': feature_cols,  # Use the feature_cols loaded from the pickle file\n    'importance': xgb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Plot top 20 most important features\nplt.figure(figsize=(10, 8))\ntop_xgb_features = xgb_importance.head(20)\nplt.barh(range(len(top_xgb_features)), top_xgb_features['importance'])\nplt.yticks(range(len(top_xgb_features)), top_xgb_features['feature'])\nplt.xlabel('Feature Importance')\nplt.title('Top 20 Most Important Features (XGBoost)')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTop 10 most important features (XGBoost):\")\nprint(xgb_importance.head(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XGBoost results\n",
    "plot_predictions(y_val, xgb_val_pred, \"XGBoost\", \"(Validation Set)\")\n",
    "plot_residuals(y_val, xgb_val_pred, \"XGBoost\", \"(Validation Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Training_R2': [lr_train_metrics['r2'], rf_train_metrics['r2'], xgb_train_metrics['r2']],\n",
    "    'Training_RMSE': [lr_train_metrics['rmse'], rf_train_metrics['rmse'], xgb_train_metrics['rmse']],\n",
    "    'Validation_R2': [lr_val_metrics['r2'], rf_val_metrics['r2'], xgb_val_metrics['r2']],\n",
    "    'Validation_RMSE': [lr_val_metrics['rmse'], rf_val_metrics['rmse'], xgb_val_metrics['rmse']],\n",
    "    'Test_R2': [lr_test_metrics['r2'], rf_test_metrics['r2'], xgb_test_metrics['r2']],\n",
    "    'Test_RMSE': [lr_test_metrics['rmse'], rf_test_metrics['rmse'], xgb_test_metrics['rmse']]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# R² comparison\n",
    "metrics = ['Training_R2', 'Validation_R2', 'Test_R2']\n",
    "x_pos = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0, 0].bar(x_pos + i*width, results_df[metric], width, \n",
    "                   label=metric.replace('_', ' '), alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].set_title('R² Score Comparison')\n",
    "axes[0, 0].set_xticks(x_pos + width)\n",
    "axes[0, 0].set_xticklabels(results_df['Model'])\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_metrics = ['Training_RMSE', 'Validation_RMSE', 'Test_RMSE']\n",
    "for i, metric in enumerate(rmse_metrics):\n",
    "    axes[0, 1].bar(x_pos + i*width, results_df[metric], width, \n",
    "                   label=metric.replace('_', ' '), alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_title('RMSE Comparison')\n",
    "axes[0, 1].set_xticks(x_pos + width)\n",
    "axes[0, 1].set_xticklabels(results_df['Model'])\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction comparison on validation set\n",
    "axes[1, 0].scatter(y_val, lr_val_pred, alpha=0.6, label='Linear Regression', s=20)\n",
    "axes[1, 0].scatter(y_val, rf_val_pred, alpha=0.6, label='Random Forest', s=20)\n",
    "axes[1, 0].scatter(y_val, xgb_val_pred, alpha=0.6, label='XGBoost', s=20)\n",
    "\n",
    "min_val = min(y_val.min(), lr_val_pred.min(), rf_val_pred.min(), xgb_val_pred.min())\n",
    "max_val = max(y_val.max(), lr_val_pred.max(), rf_val_pred.max(), xgb_val_pred.max())\n",
    "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "axes[1, 0].set_xlabel('Actual Capacity')\n",
    "axes[1, 0].set_ylabel('Predicted Capacity')\n",
    "axes[1, 0].set_title('All Models - Validation Set Predictions')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals comparison\n",
    "lr_residuals = y_val - lr_val_pred\n",
    "rf_residuals = y_val - rf_val_pred\n",
    "xgb_residuals = y_val - xgb_val_pred\n",
    "\n",
    "axes[1, 1].hist(lr_residuals, alpha=0.6, label='Linear Regression', bins=30)\n",
    "axes[1, 1].hist(rf_residuals, alpha=0.6, label='Random Forest', bins=30)\n",
    "axes[1, 1].hist(xgb_residuals, alpha=0.6, label='XGBoost', bins=30)\n",
    "axes[1, 1].set_xlabel('Residuals')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Residuals Distribution Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the best performing model on the test set for final performance assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model based on validation R²\n",
    "best_model_idx = results_df['Validation_R2'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "best_val_r2 = results_df.loc[best_model_idx, 'Validation_R2']\n",
    "best_test_r2 = results_df.loc[best_model_idx, 'Test_R2']\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Validation R²: {best_val_r2:.6f}\")\n",
    "print(f\"Test R²: {best_test_r2:.6f}\")\n",
    "\n",
    "# Get predictions from best model\n",
    "if best_model_name == 'Linear Regression':\n",
    "    best_test_pred = lr_test_pred\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_test_pred = rf_test_pred\n",
    "else:  # XGBoost\n",
    "    best_test_pred = xgb_test_pred\n",
    "\n",
    "# Final test set evaluation\n",
    "print(f\"\\nFinal Test Set Evaluation - {best_model_name}:\")\n",
    "final_metrics = evaluate_model(y_test, best_test_pred, best_model_name)\n",
    "\n",
    "# Plot final results\n",
    "plot_predictions(y_test, best_test_pred, best_model_name, \"(Test Set)\")\n",
    "plot_residuals(y_test, best_test_pred, best_model_name, \"(Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Summary and Recommendations\n",
    "\n",
    "Summary of model performance and recommendations for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModels Trained:\")\n",
    "print(\"1. Linear Regression - Simple baseline model\")\n",
    "print(\"2. Random Forest - Ensemble method with feature importance\")\n",
    "print(\"3. XGBoost - Gradient boosting with regularization\")\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "print(\"\\nPerformance Summary (Test Set R²):\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"{row['Model']}: {row['Test_R2']:.6f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test Set R²: {best_test_r2:.6f}\")\n",
    "print(f\"Test Set RMSE: {results_df.loc[best_model_idx, 'Test_RMSE']:.6f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    train_r2 = row['Training_R2']\n",
    "    val_r2 = row['Validation_R2']\n",
    "    gap = train_r2 - val_r2\n",
    "    print(f\"{row['Model']}: Training R² = {train_r2:.4f}, Validation R² = {val_r2:.4f}, Gap = {gap:.4f}\")\n",
    "    \n",
    "    if gap > 0.1:\n",
    "        print(f\"  WARNING: {row['Model']} shows signs of overfitting (gap > 0.1)\")\n",
    "    elif gap > 0.05:\n",
    "        print(f\"  CAUTION: {row['Model']} shows mild overfitting (gap > 0.05)\")\n",
    "    else:\n",
    "        print(f\"  GOOD: {row['Model']} shows good generalization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Deploy {best_model_name} for production use\")\n",
    "print(\"2. Monitor model performance on new data\")\n",
    "print(\"3. Consider hyperparameter tuning for further improvements\")\n",
    "print(\"4. Implement feature selection to reduce model complexity\")\n",
    "print(\"5. Consider ensemble methods combining multiple models\")\n",
    "\n",
    "if best_model_name != 'Linear Regression':\n",
    "    print(f\"6. Use feature importance from {best_model_name} for feature engineering\")\n",
    "\n",
    "print(\"\\nModel training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}