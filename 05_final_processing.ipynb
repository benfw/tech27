{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data Processing and Train/Val/Test Splits\n",
    "## Stanford TECH 27 Final Project - Notebook 05\n",
    "\n",
    "This notebook handles the final data processing steps, including creation of battery-based train/validation/test splits and preparation of final datasets for model training.\n",
    "It builds upon the engineered features and sequential data from previous notebooks.\n",
    "\n",
    "### Goals:\n",
    "1. Load engineered features and sequence configurations from previous notebooks\n",
    "2. Create battery-based train/validation/test splits to prevent data leakage\n",
    "3. Apply proper scaling and standardization to all datasets\n",
    "4. Prepare final datasets for both conventional ML and deep learning models\n",
    "5. Save production-ready datasets with comprehensive metadata\n",
    "\n",
    "### Output:\n",
    "- Complete processed datasets with proper train/val/test splits\n",
    "- Scaled and normalized features for ML models\n",
    "- Sequential datasets for deep learning with proper splits\n",
    "- Comprehensive metadata and data quality reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import our custom utilities\n",
    "from data_processing_utils import (\n",
    "    load_cleaned_data, engineer_features, \n",
    "    create_train_val_test_splits, prepare_ml_datasets, prepare_sequence_ml_datasets,\n",
    "    save_datasets, OUTPUT_DIR\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Results and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered features\n",
    "engineered_features_path = OUTPUT_DIR / 'engineered_features.csv'\n",
    "\n",
    "if engineered_features_path.exists():\n",
    "    print(\"Loading engineered features from 03_feature_engineering.ipynb...\")\n",
    "    engineered_data = pd.read_csv(engineered_features_path)\n",
    "    print(f\"Loaded engineered features: {engineered_data.shape}\")\n",
    "else:\n",
    "    print(\"Engineered features not found. Processing from cleaned data...\")\n",
    "    # Fallback: process from cleaned data\n",
    "    cleaned_data = load_cleaned_data()\n",
    "    \n",
    "    # Process a subset for demonstration\n",
    "    unique_batteries = cleaned_data['battery_id'].unique()\n",
    "    sample_batteries = unique_batteries[:10]  # First 10 batteries for demo\n",
    "    \n",
    "    print(f\"Processing {len(sample_batteries)} batteries for demonstration...\")\n",
    "    engineered_list = []\n",
    "    \n",
    "    for battery_id in tqdm(sample_batteries, desc=\"Engineering features\"):\n",
    "        battery_data = cleaned_data[cleaned_data['battery_id'] == battery_id].copy()\n",
    "        try:\n",
    "            engineered_battery = engineer_features(battery_data)\n",
    "            engineered_list.append(engineered_battery)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {battery_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if engineered_list:\n",
    "        engineered_data = pd.concat(engineered_list, ignore_index=True)\n",
    "        print(f\"Created sample engineered features: {engineered_data.shape}\")\n",
    "    else:\n",
    "        raise Exception(\"No data could be processed\")\n",
    "\n",
    "print(f\"\\nEngineered data overview:\")\n",
    "print(f\"Shape: {engineered_data.shape}\")\n",
    "print(f\"SOC range: {engineered_data['SOC'].min():.3f} - {engineered_data['SOC'].max():.3f}\")\n",
    "print(f\"Unique batteries: {engineered_data['battery_id'].nunique()}\")\n",
    "print(f\"Unique files: {engineered_data['filename'].nunique() if 'filename' in engineered_data.columns else 'N/A'}\")\n",
    "\n",
    "# Load sequence preparation metadata if available\n",
    "sequence_config = {'lstm': {'sequence_length': 30, 'step': 5}, 'cnn': {'sequence_length': 80, 'step': 10}}\n",
    "metadata_path = OUTPUT_DIR / 'sequence_preparation_metadata.pkl'\n",
    "\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        sequence_metadata = pickle.load(f)\n",
    "    sequence_config = sequence_metadata.get('recommended_configs', sequence_config)\n",
    "    print(f\"\\nLoaded sequence configurations:\")\n",
    "    print(f\"  LSTM: length={sequence_config['lstm']['sequence_length']}, step={sequence_config['lstm']['step']}\")\n",
    "    print(f\"  CNN: length={sequence_config['cnn']['sequence_length']}, step={sequence_config['cnn']['step']}\")\n",
    "else:\n",
    "    print(f\"\\nUsing default sequence configurations:\")\n",
    "    print(f\"  LSTM: length={sequence_config['lstm']['sequence_length']}, step={sequence_config['lstm']['step']}\")\n",
    "    print(f\"  CNN: length={sequence_config['cnn']['sequence_length']}, step={sequence_config['cnn']['step']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality analysis\n",
    "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"  Total data points: {len(engineered_data):,}\")\n",
    "print(f\"  Unique batteries: {engineered_data['battery_id'].nunique()}\")\n",
    "print(f\"  Features: {len([col for col in engineered_data.columns if col not in ['SOC', 'battery_id', 'filename', 'test_id', 'ambient_temperature', 'time']])}\")\n",
    "\n",
    "# Battery distribution\n",
    "battery_counts = engineered_data.groupby('battery_id').size().sort_values(ascending=False)\n",
    "print(f\"\\nData distribution by battery:\")\n",
    "print(f\"  Mean data points per battery: {battery_counts.mean():.1f}\")\n",
    "print(f\"  Min data points per battery: {battery_counts.min()}\")\n",
    "print(f\"  Max data points per battery: {battery_counts.max()}\")\n",
    "print(f\"  Std data points per battery: {battery_counts.std():.1f}\")\n",
    "\n",
    "# SOC distribution\n",
    "soc_stats = engineered_data['SOC'].describe()\n",
    "print(f\"\\nSOC Distribution:\")\n",
    "print(f\"  Range: {soc_stats['min']:.3f} to {soc_stats['max']:.3f}\")\n",
    "print(f\"  Mean: {soc_stats['mean']:.3f} Â± {soc_stats['std']:.3f}\")\n",
    "print(f\"  Median: {soc_stats['50%']:.3f}\")\n",
    "\n",
    "# Feature quality check\n",
    "feature_cols = [col for col in engineered_data.columns \n",
    "                if col not in ['SOC', 'battery_id', 'filename', 'test_id', 'ambient_temperature', 'time']]\n",
    "\n",
    "if feature_cols:\n",
    "    nan_counts = engineered_data[feature_cols].isnull().sum()\n",
    "    inf_counts = np.isinf(engineered_data[feature_cols]).sum()\n",
    "    \n",
    "    print(f\"\\nFeature Quality:\")\n",
    "    print(f\"  Total features: {len(feature_cols)}\")\n",
    "    print(f\"  Features with NaN: {(nan_counts > 0).sum()}\")\n",
    "    print(f\"  Features with Inf: {(inf_counts > 0).sum()}\")\n",
    "    print(f\"  Total NaN values: {nan_counts.sum()}\")\n",
    "    print(f\"  Total Inf values: {inf_counts.sum()}\")\n",
    "    \n",
    "    if (nan_counts > 0).sum() > 0:\n",
    "        print(f\"  Features with most NaN: {nan_counts.nlargest(5).to_dict()}\")\n",
    "    \n",
    "    if (inf_counts > 0).sum() > 0:\n",
    "        print(f\"  Features with most Inf: {inf_counts.nlargest(5).to_dict()}\")\n",
    "\n",
    "# Visualize data quality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Data Quality Analysis', fontsize=16)\n",
    "\n",
    "# SOC distribution\n",
    "axes[0, 0].hist(engineered_data['SOC'] * 100, bins=50, alpha=0.7, color='blue')\n",
    "axes[0, 0].set_title('SOC Distribution')\n",
    "axes[0, 0].set_xlabel('SOC (%)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Battery data distribution\n",
    "battery_counts.head(15).plot(kind='bar', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Data Points by Battery (Top 15)')\n",
    "axes[0, 1].set_xlabel('Battery ID')\n",
    "axes[0, 1].set_ylabel('Data Points')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Voltage vs SOC sample\n",
    "sample_size = min(10000, len(engineered_data))\n",
    "sample_indices = np.random.choice(len(engineered_data), sample_size, replace=False)\n",
    "sample_df = engineered_data.iloc[sample_indices]\n",
    "\n",
    "if 'voltage' in sample_df.columns:\n",
    "    axes[1, 0].scatter(sample_df['voltage'], sample_df['SOC'] * 100, alpha=0.5, s=1)\n",
    "    axes[1, 0].set_title('Voltage vs SOC (Sample)')\n",
    "    axes[1, 0].set_xlabel('Voltage (V)')\n",
    "    axes[1, 0].set_ylabel('SOC (%)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Voltage\\nfeature\\nnot found', ha='center', va='center', \n",
    "                   transform=axes[1, 0].transAxes, fontsize=14)\n",
    "    axes[1, 0].set_title('Voltage vs SOC')\n",
    "\n",
    "# Battery count histogram\n",
    "axes[1, 1].hist(battery_counts, bins=20, alpha=0.7, color='green')\n",
    "axes[1, 1].set_title('Distribution of Data Points per Battery')\n",
    "axes[1, 1].set_xlabel('Data Points per Battery')\n",
    "axes[1, 1].set_ylabel('Number of Batteries')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create battery-based train/validation/test splits\n",
    "print(\"Creating battery-based train/validation/test splits...\")\n",
    "print(\"This prevents data leakage by ensuring batteries are not split across sets.\")\n",
    "\n",
    "data_splits = create_train_val_test_splits(\n",
    "    engineered_data, \n",
    "    test_size=0.2, \n",
    "    val_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Analyze splits\n",
    "print(f\"\\n=== SPLIT ANALYSIS ===\")\n",
    "for split_name, split_data in data_splits.items():\n",
    "    soc_stats = split_data['SOC'].describe()\n",
    "    print(f\"{split_name.upper():5s}: {len(split_data):7,d} samples, {split_data['battery_id'].nunique():2d} batteries\")\n",
    "    print(f\"       SOC range: {soc_stats['min']:.3f}-{soc_stats['max']:.3f}, mean: {soc_stats['mean']:.3f}Â±{soc_stats['std']:.3f}\")\n",
    "\n",
    "# Visualize splits\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Train/Validation/Test Split Analysis', fontsize=16)\n",
    "\n",
    "# Split proportions\n",
    "split_sizes = [len(data_splits['train']), len(data_splits['val']), len(data_splits['test'])]\n",
    "split_labels = ['Train', 'Validation', 'Test']\n",
    "colors = ['blue', 'orange', 'red']\n",
    "\n",
    "axes[0, 0].pie(split_sizes, labels=split_labels, autopct='%1.1f%%', colors=colors)\n",
    "axes[0, 0].set_title('Data Split Proportions')\n",
    "\n",
    "# SOC distribution by split\n",
    "for i, (split_name, split_data) in enumerate(data_splits.items()):\n",
    "    axes[0, 1].hist(split_data['SOC'] * 100, bins=30, alpha=0.7, \n",
    "                   label=split_name.capitalize(), color=colors[i])\n",
    "axes[0, 1].set_title('SOC Distribution by Split')\n",
    "axes[0, 1].set_xlabel('SOC (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Battery distribution by split\n",
    "battery_split_counts = [data_splits['train']['battery_id'].nunique(),\n",
    "                       data_splits['val']['battery_id'].nunique(), \n",
    "                       data_splits['test']['battery_id'].nunique()]\n",
    "\n",
    "axes[1, 0].bar(split_labels, battery_split_counts, color=colors)\n",
    "axes[1, 0].set_title('Number of Batteries by Split')\n",
    "axes[1, 0].set_ylabel('Number of Batteries')\n",
    "\n",
    "# Voltage vs SOC by split (if voltage available)\n",
    "if 'voltage' in engineered_data.columns:\n",
    "    for i, (split_name, split_data) in enumerate(data_splits.items()):\n",
    "        # Sample for visualization\n",
    "        sample_size = min(1000, len(split_data))\n",
    "        if sample_size > 0:\n",
    "            sample_indices = np.random.choice(len(split_data), sample_size, replace=False)\n",
    "            sample = split_data.iloc[sample_indices]\n",
    "            axes[1, 1].scatter(sample['voltage'], sample['SOC'] * 100, \n",
    "                             alpha=0.6, s=1, label=split_name.capitalize(), c=colors[i])\n",
    "    axes[1, 1].set_title('Voltage vs SOC by Split (Sample)')\n",
    "    axes[1, 1].set_xlabel('Voltage (V)')\n",
    "    axes[1, 1].set_ylabel('SOC (%)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Voltage\\nfeature\\nnot available', ha='center', va='center',\n",
    "                   transform=axes[1, 1].transAxes, fontsize=14)\n",
    "    axes[1, 1].set_title('Voltage vs SOC by Split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare ML Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for conventional ML models\n",
    "print(\"Preparing ML datasets with feature scaling...\")\n",
    "\n",
    "ml_datasets = prepare_ml_datasets(\n",
    "    data_splits, \n",
    "    target_col='SOC', \n",
    "    scale_features=True\n",
    ")\n",
    "\n",
    "print(f\"\\n=== ML DATASET SHAPES ===\")\n",
    "print(f\"Training:   X={ml_datasets['X_train'].shape}, y={ml_datasets['y_train'].shape}\")\n",
    "print(f\"Validation: X={ml_datasets['X_val'].shape}, y={ml_datasets['y_val'].shape}\")\n",
    "print(f\"Test:       X={ml_datasets['X_test'].shape}, y={ml_datasets['y_test'].shape}\")\n",
    "print(f\"Features:   {len(ml_datasets['feature_cols'])}\")\n",
    "\n",
    "# Data quality checks for ML datasets\n",
    "print(f\"\\n=== ML DATA QUALITY CHECKS ===\")\n",
    "print(f\"NaN values in X_train: {ml_datasets['X_train'].isnull().sum().sum()}\")\n",
    "print(f\"Infinite values in X_train: {np.isinf(ml_datasets['X_train']).sum().sum()}\")\n",
    "print(f\"Feature mean (should be ~0): {ml_datasets['X_train'].mean().mean():.6f}\")\n",
    "print(f\"Feature std (should be ~1): {ml_datasets['X_train'].std().mean():.6f}\")\n",
    "\n",
    "# Target statistics\n",
    "print(f\"\\n=== TARGET STATISTICS ===\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    y_data = ml_datasets[f'y_{split}']\n",
    "    print(f\"{split.upper():5s}: mean={y_data.mean():.3f}, std={y_data.std():.3f}, range=[{y_data.min():.3f}, {y_data.max():.3f}]\")\n",
    "\n",
    "# Show feature names\n",
    "print(f\"\\nFirst 10 features: {ml_datasets['feature_cols'][:10]}\")\n",
    "if len(ml_datasets['feature_cols']) > 10:\n",
    "    print(f\"... and {len(ml_datasets['feature_cols']) - 10} more features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Sequential Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequential datasets for deep learning models\n",
    "print(\"Preparing sequential datasets for deep learning...\")\n",
    "\n",
    "# LSTM sequences\n",
    "print(f\"\\nCreating LSTM sequences (length={sequence_config['lstm']['sequence_length']}, step={sequence_config['lstm']['step']})...\")\n",
    "lstm_datasets = prepare_sequence_ml_datasets(\n",
    "    data_splits,\n",
    "    sequence_length=sequence_config['lstm']['sequence_length'],\n",
    "    step=sequence_config['lstm']['step'],\n",
    "    target_col='SOC'\n",
    ")\n",
    "\n",
    "# CNN sequences  \n",
    "print(f\"\\nCreating CNN sequences (length={sequence_config['cnn']['sequence_length']}, step={sequence_config['cnn']['step']})...\")\n",
    "cnn_datasets = prepare_sequence_ml_datasets(\n",
    "    data_splits,\n",
    "    sequence_length=sequence_config['cnn']['sequence_length'], \n",
    "    step=sequence_config['cnn']['step'],\n",
    "    target_col='SOC'\n",
    ")\n",
    "\n",
    "# Analysis of sequence datasets\n",
    "sequence_results = {\n",
    "    'LSTM': lstm_datasets,\n",
    "    'CNN': cnn_datasets\n",
    "}\n",
    "\n",
    "print(f\"\\n=== SEQUENCE DATASET ANALYSIS ===\")\n",
    "for model_type, datasets in sequence_results.items():\n",
    "    if 'X_train' in datasets and len(datasets['X_train']) > 0:\n",
    "        print(f\"\\n{model_type} Sequences:\")\n",
    "        print(f\"  Train: X={datasets['X_train'].shape}, y={datasets['y_train'].shape}\")\n",
    "        print(f\"  Val:   X={datasets['X_val'].shape}, y={datasets['y_val'].shape}\")\n",
    "        print(f\"  Test:  X={datasets['X_test'].shape}, y={datasets['y_test'].shape}\")\n",
    "        print(f\"  Memory: {datasets['X_train'].nbytes / 1024**2:.2f} MB (train)\")\n",
    "        print(f\"  Features: {len(datasets['feature_names'])}\")\n",
    "        \n",
    "        # Target statistics for sequences\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            y_seq = datasets[f'y_{split}']\n",
    "            if len(y_seq) > 0:\n",
    "                print(f\"    {split}: SOC range [{y_seq.min():.3f}, {y_seq.max():.3f}], mean={y_seq.mean():.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n{model_type}: No sequences generated (insufficient data)\")\n",
    "\n",
    "# Visualize sequence characteristics\n",
    "if len(lstm_datasets.get('X_train', [])) > 0 or len(cnn_datasets.get('X_train', [])) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Sequential Dataset Analysis', fontsize=16)\n",
    "    \n",
    "    # Sequence counts by model type and split\n",
    "    model_names = []\n",
    "    train_counts = []\n",
    "    val_counts = []\n",
    "    test_counts = []\n",
    "    \n",
    "    for model_type, datasets in sequence_results.items():\n",
    "        if 'X_train' in datasets and len(datasets['X_train']) > 0:\n",
    "            model_names.append(model_type)\n",
    "            train_counts.append(len(datasets['X_train']))\n",
    "            val_counts.append(len(datasets['X_val']))\n",
    "            test_counts.append(len(datasets['X_test']))\n",
    "    \n",
    "    if model_names:\n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        axes[0, 0].bar(x - width, train_counts, width, label='Train', color='blue', alpha=0.7)\n",
    "        axes[0, 0].bar(x, val_counts, width, label='Validation', color='orange', alpha=0.7)\n",
    "        axes[0, 0].bar(x + width, test_counts, width, label='Test', color='red', alpha=0.7)\n",
    "        axes[0, 0].set_title('Sequence Counts by Model Type')\n",
    "        axes[0, 0].set_ylabel('Number of Sequences')\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels(model_names)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SOC distribution for LSTM sequences\n",
    "    if len(lstm_datasets.get('X_train', [])) > 0:\n",
    "        axes[0, 1].hist(lstm_datasets['y_train'] * 100, bins=30, alpha=0.7, \n",
    "                       color='blue', label='LSTM Train')\n",
    "        axes[0, 1].set_title('LSTM Sequence SOC Distribution')\n",
    "        axes[0, 1].set_xlabel('SOC (%)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No LSTM\\nSequences', ha='center', va='center',\n",
    "                       transform=axes[0, 1].transAxes, fontsize=14)\n",
    "        axes[0, 1].set_title('LSTM Sequence SOC Distribution')\n",
    "    \n",
    "    # SOC distribution for CNN sequences\n",
    "    if len(cnn_datasets.get('X_train', [])) > 0:\n",
    "        axes[1, 0].hist(cnn_datasets['y_train'] * 100, bins=30, alpha=0.7, \n",
    "                       color='red', label='CNN Train')\n",
    "        axes[1, 0].set_title('CNN Sequence SOC Distribution')\n",
    "        axes[1, 0].set_xlabel('SOC (%)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No CNN\\nSequences', ha='center', va='center',\n",
    "                       transform=axes[1, 0].transAxes, fontsize=14)\n",
    "        axes[1, 0].set_title('CNN Sequence SOC Distribution')\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    if model_names:\n",
    "        memory_usage = []\n",
    "        for model_type, datasets in sequence_results.items():\n",
    "            if 'X_train' in datasets and len(datasets['X_train']) > 0:\n",
    "                memory_mb = datasets['X_train'].nbytes / 1024**2\n",
    "                memory_usage.append(memory_mb)\n",
    "        \n",
    "        axes[1, 1].bar(model_names, memory_usage, color=['blue', 'red'][:len(model_names)], alpha=0.7)\n",
    "        axes[1, 1].set_title('Memory Usage by Model Type')\n",
    "        axes[1, 1].set_ylabel('Memory (MB)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No Sequence\\nData', ha='center', va='center',\n",
    "                       transform=axes[1, 1].transAxes, fontsize=14)\n",
    "        axes[1, 1].set_title('Memory Usage by Model Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all final datasets\n",
    "print(\"Saving final datasets...\")\n",
    "\n",
    "# Save ML datasets\n",
    "ml_paths = save_datasets(ml_datasets, output_prefix=\"final_ml\")\n",
    "print(f\"â Saved ML datasets\")\n",
    "\n",
    "# Save LSTM datasets if available\n",
    "lstm_paths = None\n",
    "if len(lstm_datasets.get('X_train', [])) > 0:\n",
    "    lstm_paths = save_datasets(lstm_datasets, output_prefix=\"final_lstm\")\n",
    "    print(f\"â Saved LSTM sequence datasets\")\n",
    "else:\n",
    "    print(f\"â ï¸  No LSTM sequences to save\")\n",
    "\n",
    "# Save CNN datasets if available\n",
    "cnn_paths = None\n",
    "if len(cnn_datasets.get('X_train', [])) > 0:\n",
    "    cnn_paths = save_datasets(cnn_datasets, output_prefix=\"final_cnn\")\n",
    "    print(f\"â Saved CNN sequence datasets\")\n",
    "else:\n",
    "    print(f\"â ï¸  No CNN sequences to save\")\n",
    "\n",
    "# Save comprehensive metadata\n",
    "final_metadata = {\n",
    "    'processing_date': pd.Timestamp.now().isoformat(),\n",
    "    'data_source': 'NASA Battery Dataset',\n",
    "    'total_samples': len(engineered_data),\n",
    "    'unique_batteries': engineered_data['battery_id'].nunique(),\n",
    "    'features_engineered': len(ml_datasets['feature_cols']),\n",
    "    \n",
    "    # Split information\n",
    "    'train_val_test_splits': {\n",
    "        'method': 'battery_based',\n",
    "        'train_size': len(data_splits['train']),\n",
    "        'val_size': len(data_splits['val']),\n",
    "        'test_size': len(data_splits['test']),\n",
    "        'train_batteries': data_splits['train']['battery_id'].nunique(),\n",
    "        'val_batteries': data_splits['val']['battery_id'].nunique(),\n",
    "        'test_batteries': data_splits['test']['battery_id'].nunique()\n",
    "    },\n",
    "    \n",
    "    # ML dataset info\n",
    "    'ml_datasets': {\n",
    "        'feature_scaling': 'StandardScaler',\n",
    "        'train_shape': ml_datasets['X_train'].shape,\n",
    "        'val_shape': ml_datasets['X_val'].shape,\n",
    "        'test_shape': ml_datasets['X_test'].shape,\n",
    "        'feature_names': ml_datasets['feature_cols']\n",
    "    },\n",
    "    \n",
    "    # Target statistics\n",
    "    'target_statistics': {\n",
    "        'train': {\n",
    "            'mean': float(ml_datasets['y_train'].mean()),\n",
    "            'std': float(ml_datasets['y_train'].std()),\n",
    "            'min': float(ml_datasets['y_train'].min()),\n",
    "            'max': float(ml_datasets['y_train'].max())\n",
    "        },\n",
    "        'val': {\n",
    "            'mean': float(ml_datasets['y_val'].mean()),\n",
    "            'std': float(ml_datasets['y_val'].std()),\n",
    "            'min': float(ml_datasets['y_val'].min()),\n",
    "            'max': float(ml_datasets['y_val'].max())\n",
    "        },\n",
    "        'test': {\n",
    "            'mean': float(ml_datasets['y_test'].mean()),\n",
    "            'std': float(ml_datasets['y_test'].std()),\n",
    "            'min': float(ml_datasets['y_test'].min()),\n",
    "            'max': float(ml_datasets['y_test'].max())\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Sequence configurations\n",
    "    'sequence_configs': sequence_config,\n",
    "    \n",
    "    # File paths\n",
    "    'output_files': {\n",
    "        'ml_datasets': str(ml_paths['ml_path']),\n",
    "        'lstm_datasets': str(lstm_paths['ml_path']) if lstm_paths else None,\n",
    "        'cnn_datasets': str(cnn_paths['ml_path']) if cnn_paths else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add sequence dataset info if available\n",
    "if len(lstm_datasets.get('X_train', [])) > 0:\n",
    "    final_metadata['lstm_datasets'] = {\n",
    "        'sequence_length': sequence_config['lstm']['sequence_length'],\n",
    "        'step_size': sequence_config['lstm']['step'],\n",
    "        'train_shape': lstm_datasets['X_train'].shape,\n",
    "        'val_shape': lstm_datasets['X_val'].shape,\n",
    "        'test_shape': lstm_datasets['X_test'].shape,\n",
    "        'feature_names': lstm_datasets['feature_names']\n",
    "    }\n",
    "\n",
    "if len(cnn_datasets.get('X_train', [])) > 0:\n",
    "    final_metadata['cnn_datasets'] = {\n",
    "        'sequence_length': sequence_config['cnn']['sequence_length'],\n",
    "        'step_size': sequence_config['cnn']['step'],\n",
    "        'train_shape': cnn_datasets['X_train'].shape,\n",
    "        'val_shape': cnn_datasets['X_val'].shape,\n",
    "        'test_shape': cnn_datasets['X_test'].shape,\n",
    "        'feature_names': cnn_datasets['feature_names']\n",
    "    }\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = OUTPUT_DIR / 'final_processing_metadata.pkl'\n",
    "with open(metadata_file, 'wb') as f:\n",
    "    pickle.dump(final_metadata, f)\n",
    "\n",
    "print(f\"\\nâ Saved comprehensive metadata to {metadata_file}\")\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ð FINAL DATA PROCESSING COMPLETE! ð\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"\\nð FINAL DATASET SUMMARY:\")\n",
    "print(f\"   Total samples processed: {len(engineered_data):,}\")\n",
    "print(f\"   Unique batteries: {engineered_data['battery_id'].nunique()}\")\n",
    "print(f\"   Features engineered: {len(ml_datasets['feature_cols'])}\")\n",
    "print(f\"   Train/Val/Test split: {len(data_splits['train'])}/{len(data_splits['val'])}/{len(data_splits['test'])}\")\n",
    "\n",
    "print(f\"\\nð OUTPUT FILES:\")\n",
    "print(f\"   ML datasets: {ml_paths['ml_path']}\")\n",
    "if lstm_paths:\n",
    "    print(f\"   LSTM sequences: {lstm_paths['ml_path']}\")\n",
    "if cnn_paths:\n",
    "    print(f\"   CNN sequences: {cnn_paths['ml_path']}\")\n",
    "print(f\"   Metadata: {metadata_file}\")\n",
    "\n",
    "# File sizes\n",
    "print(f\"\\nð¾ FILE SIZES:\")\n",
    "for file_path in [ml_paths['ml_path']] + ([lstm_paths['ml_path']] if lstm_paths else []) + ([cnn_paths['ml_path']] if cnn_paths else []) + [metadata_file]:\n",
    "    if Path(file_path).exists():\n",
    "        size_mb = Path(file_path).stat().st_size / 1024**2\n",
    "        print(f\"   {Path(file_path).name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nð READY FOR MODEL TRAINING!\")\n",
    "print(f\"   Next: Create model training notebooks using these datasets\")\n",
    "print(f\"   Available models: Linear Regression, Random Forest, XGBoost, LSTM, CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully completed the final data processing pipeline:\n",
    "\n",
    "### â Completed Tasks:\n",
    "1. **Data Loading**: Loaded engineered features from `03_feature_engineering.ipynb`\n",
    "2. **Quality Analysis**: Comprehensive data quality assessment and visualization\n",
    "3. **Train/Val/Test Splits**: Created battery-based splits to prevent data leakage\n",
    "4. **ML Dataset Preparation**: Scaled and normalized features for conventional models\n",
    "5. **Sequential Dataset Preparation**: Generated LSTM and CNN sequences with proper splits\n",
    "6. **Production Output**: Saved all datasets with comprehensive metadata\n",
    "\n",
    "### ð Data Quality Measures:\n",
    "- **Battery-based Splits**: Proper separation ensures realistic model evaluation\n",
    "- **Feature Scaling**: StandardScaler applied for optimal ML performance\n",
    "- **Sequential Structure**: Temporal relationships preserved for deep learning\n",
    "- **Comprehensive Validation**: Quality checks throughout the pipeline\n",
    "\n",
    "### ð Final Dataset Characteristics:\n",
    "- **Tabular ML Data**: Scaled features for conventional machine learning\n",
    "- **LSTM Sequences**: Optimized temporal sequences for recurrent networks\n",
    "- **CNN Sequences**: Longer sequences for convolutional pattern recognition\n",
    "- **Proper Splits**: Battery-based separation for realistic evaluation\n",
    "\n",
    "### ð¯ Key Innovations:\n",
    "1. **Battery-based Splitting**: Prevents data leakage in real-world scenarios\n",
    "2. **Multi-model Support**: Datasets optimized for different architectures\n",
    "3. **Comprehensive Metadata**: Full traceability and reproducibility\n",
    "4. **Production Ready**: Standardized format for immediate model training\n",
    "5. **Quality Assurance**: Multiple validation layers ensure data integrity\n",
    "\n",
    "### ð Final Outputs:\n",
    "- **`final_ml_ml_datasets.pkl`**: Complete ML datasets with train/val/test splits\n",
    "- **`final_lstm_ml_datasets.pkl`**: LSTM sequence datasets (if generated)\n",
    "- **`final_cnn_ml_datasets.pkl`**: CNN sequence datasets (if generated)\n",
    "- **`final_processing_metadata.pkl`**: Comprehensive processing metadata\n",
    "\n",
    "### ð Next Phase:\n",
    "The preprocessing pipeline is complete! The generated datasets are ready for:\n",
    "1. **Conventional ML Models**: Linear Regression, Random Forest, XGBoost, SVR\n",
    "2. **Deep Learning Models**: LSTM, GRU, 1D CNN, Transformer architectures\n",
    "3. **Ensemble Methods**: Combining multiple model approaches\n",
    "4. **Comprehensive Evaluation**: Robust testing on battery-separated test sets\n",
    "\n",
    "This modular approach enables rapid experimentation with different model architectures while maintaining data integrity and preventing overfitting through proper train/validation/test separation based on battery IDs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}