{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: GRU (Gated Recurrent Unit)\n",
    "## Stanford TECH 27 Final Project - Notebook 09\n",
    "\n",
    "This notebook trains a GRU model for SOC estimation using the sequential data prepared in notebooks 04 and 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from data_processing_utils import filter_features, get_realistic_features\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GRU Sequential Data\n",
    "\n",
    "Load the LSTM sequential datasets from notebook 05 (we'll use the same data format as LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM sequential data from notebook 05_final_processing.ipynb (same format for GRU)\n",
    "OUTPUT_DIR = Path('processed_data')\n",
    "gru_datasets_path = OUTPUT_DIR / 'final_lstm_ml_datasets.pkl'\n",
    "\n",
    "if gru_datasets_path.exists():\n",
    "    print(f\"Loading GRU sequential datasets from {gru_datasets_path}...\")\n",
    "    with open(gru_datasets_path, 'rb') as f:\n",
    "        gru_datasets = pickle.load(f)\n",
    "    \n",
    "    # Extract the full datasets\n",
    "    X_train_full = gru_datasets['X_train']\n",
    "    X_val_full = gru_datasets['X_val']\n",
    "    X_test_full = gru_datasets['X_test']\n",
    "    y_train = gru_datasets['y_train']\n",
    "    y_val = gru_datasets['y_val']\n",
    "    y_test = gru_datasets['y_test']\n",
    "    \n",
    "    # Get all feature names\n",
    "    all_feature_names = gru_datasets.get('feature_names', \n",
    "                                          [f'feature_{i}' for i in range(X_train_full.shape[2])])\n",
    "    \n",
    "    print(f\"✅ Successfully loaded GRU datasets from notebook 5\")\n",
    "    print(f\"Full dataset shapes:\")\n",
    "    print(f\"   Training:   X={X_train_full.shape}, y={y_train.shape}\")\n",
    "    print(f\"   Validation: X={X_val_full.shape}, y={y_val.shape}\")\n",
    "    print(f\"   Test:       X={X_test_full.shape}, y={y_test.shape}\")\n",
    "    print(f\"\\nSequence length: {X_train_full.shape[1]} timesteps\")\n",
    "    print(f\"Number of features: {X_train_full.shape[2]}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ Error: Could not find {gru_datasets_path}\")\n",
    "    print(\"Please run notebook 05_final_processing.ipynb first to generate the sequential datasets.\")\n",
    "    raise FileNotFoundError(f\"Dataset file not found: {gru_datasets_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Features\n",
    "\n",
    "Filter the dataset to use only the realistic features available from V, I, T measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the standard list of realistic features from centralized function\n",
    "features = get_realistic_features()\n",
    "\n",
    "# Use the centralized filter_features function to filter datasets\n",
    "filtered_data = filter_features(\n",
    "    X_train_full, X_val_full, X_test_full,\n",
    "    all_feature_names, features,\n",
    "    y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Extract the filtered datasets\n",
    "X_train = filtered_data['X_train']\n",
    "X_val = filtered_data['X_val']\n",
    "X_test = filtered_data['X_test']\n",
    "available_realistic = filtered_data['available_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Filtered Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Convert to percentage for SOC\n",
    "    rmse_percent = rmse * 100\n",
    "    mae_percent = mae * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"  R² Score: {r2:.6f}\")\n",
    "    print(f\"  RMSE: {rmse:.6f} ({rmse_percent:.3f}% SOC)\")\n",
    "    print(f\"  MAE: {mae:.6f} ({mae_percent:.3f}% SOC)\")\n",
    "    \n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2,\n",
    "            'rmse_percent': rmse_percent, 'mae_percent': mae_percent}\n",
    "\n",
    "def plot_predictions(y_true, y_pred, model_name, dataset_name=''):\n",
    "    \"\"\"Plot predicted vs actual values.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Sample for visualization if too many points\n",
    "    if len(y_true) > 10000:\n",
    "        sample_idx = np.random.choice(len(y_true), 10000, replace=False)\n",
    "        y_true_sample = y_true[sample_idx]\n",
    "        y_pred_sample = y_pred[sample_idx]\n",
    "    else:\n",
    "        y_true_sample = y_true\n",
    "        y_pred_sample = y_pred\n",
    "    \n",
    "    plt.scatter(y_true_sample * 100, y_pred_sample * 100, alpha=0.6, s=1)\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    plt.plot([0, 100], [0, 100], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel('Actual SOC (%)')\n",
    "    plt.ylabel('Predicted SOC (%)')\n",
    "    plt.title(f'{model_name} - Predicted vs Actual {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R² score to plot\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse_percent = np.sqrt(mean_squared_error(y_true, y_pred)) * 100\n",
    "    plt.text(0.05, 0.95, f'R² = {r2:.4f}\\nRMSE = {rmse_percent:.2f}%', \n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation loss over epochs.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MSE)')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE plot\n",
    "    if 'mae' in history.history:\n",
    "        axes[1].plot(history.history['mae'], label='Training MAE')\n",
    "        axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].set_title('Training and Validation MAE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(sequence_length, n_features, gru_units=[48, 24], \n",
    "                    dropout_rate=0.3, l2_reg=0.005):\n",
    "    \"\"\"\n",
    "    Build a GRU model for SOC estimation.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of input sequences\n",
    "        n_features: Number of features\n",
    "        gru_units: List of GRU unit sizes\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        l2_reg: L2 regularization strength\n",
    "    \"\"\"\n",
    "    from tensorflow.keras import regularizers\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(sequence_length, n_features)),\n",
    "        \n",
    "        # Masking layer for padded sequences if needed\n",
    "        layers.Masking(mask_value=0.0),\n",
    "        \n",
    "        # First GRU layer\n",
    "        layers.GRU(gru_units[0], \n",
    "                   return_sequences=True,\n",
    "                   dropout=dropout_rate,\n",
    "                   recurrent_dropout=dropout_rate * 0.5,\n",
    "                   kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Second GRU layer\n",
    "        layers.GRU(gru_units[1],\n",
    "                   dropout=dropout_rate,\n",
    "                   recurrent_dropout=dropout_rate * 0.5,\n",
    "                   kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Dense layer\n",
    "        layers.Dense(24, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1, activation='sigmoid')  # Sigmoid for SOC in range [0, 1]\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "sequence_length = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "print(\"Building GRU model...\")\n",
    "gru_model = build_gru_model(\n",
    "    sequence_length=sequence_length,\n",
    "    n_features=n_features,\n",
    "    gru_units=[48, 24],\n",
    "    dropout_rate=0.3,\n",
    "    l2_reg=0.005\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "gru_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "gru_model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = gru_model.count_params()\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING GRU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=12,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=6,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    OUTPUT_DIR / 'best_gru_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 512\n",
    "epochs = 120\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Max epochs: {epochs}\")\n",
    "print(f\"   Early stopping patience: 12\")\n",
    "print(f\"   Learning rate: 0.0005\")\n",
    "print(f\"   Learning rate reduction factor: 0.5\")\n",
    "print(f\"   Dropout rate: 0.3\")\n",
    "print(f\"   Recurrent dropout: 0.15\")\n",
    "print(f\"   L2 regularization: 0.005\")\n",
    "print(f\"   GRU units: [48, 24]\")\n",
    "print(f\"   Architecture: 2 GRU layers + 1 Dense layer\")\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nStarting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Best epoch: {np.argmin(history.history['val_loss']) + 1}\")\n",
    "print(f\"Best validation loss: {np.min(history.history['val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Additional training metrics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"   Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"   Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"   Final training MAE: {history.history['mae'][-1]:.6f}\")\n",
    "print(f\"   Final validation MAE: {history.history['val_mae'][-1]:.6f}\")\n",
    "print(f\"   Total epochs trained: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "gru_train_pred = gru_model.predict(X_train, batch_size=batch_size, verbose=0).flatten()\n",
    "gru_val_pred = gru_model.predict(X_val, batch_size=batch_size, verbose=0).flatten()\n",
    "gru_test_pred = gru_model.predict(X_test, batch_size=batch_size, verbose=0).flatten()\n",
    "\n",
    "# Evaluate performance\n",
    "gru_train_metrics = evaluate_model(y_train, gru_train_pred, \"GRU (Training)\")\n",
    "gru_val_metrics = evaluate_model(y_val, gru_val_pred, \"GRU (Validation)\")\n",
    "gru_test_metrics = evaluate_model(y_test, gru_test_pred, \"GRU (Test)\")\n",
    "\n",
    "print(\"\\nGRU evaluation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "plot_predictions(y_val, gru_val_pred, \"GRU\", \"(Validation Set)\")\n",
    "\n",
    "# Visualize predictions on test set\n",
    "plot_predictions(y_test, gru_test_pred, \"GRU\", \"(Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y_true, y_pred, model_name, dataset_name=''):\n",
    "    \"\"\"Plot residuals to check for patterns.\"\"\"\n",
    "    residuals = (y_true - y_pred) * 100  # Convert to percentage\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sample for visualization if too many points\n",
    "    if len(y_true) > 10000:\n",
    "        sample_idx = np.random.choice(len(y_true), 10000, replace=False)\n",
    "        y_pred_sample = y_pred[sample_idx]\n",
    "        residuals_sample = residuals[sample_idx]\n",
    "    else:\n",
    "        y_pred_sample = y_pred\n",
    "        residuals_sample = residuals\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[0].scatter(y_pred_sample * 100, residuals_sample, alpha=0.6, s=1)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Predicted SOC (%)')\n",
    "    axes[0].set_ylabel('Residuals (% SOC)')\n",
    "    axes[0].set_title(f'{model_name} - Residuals vs Predicted {dataset_name}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals histogram\n",
    "    axes[1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Residuals (% SOC)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'{model_name} - Residuals Distribution {dataset_name}')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_res = np.mean(residuals)\n",
    "    std_res = np.std(residuals)\n",
    "    axes[1].axvline(x=mean_res, color='red', linestyle='--', label=f'Mean: {mean_res:.3f}%')\n",
    "    axes[1].axvline(x=mean_res + std_res, color='orange', linestyle=':', label=f'±1σ: {std_res:.3f}%')\n",
    "    axes[1].axvline(x=mean_res - std_res, color='orange', linestyle=':')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot residuals for validation and test sets\n",
    "plot_residuals(y_val, gru_val_pred, \"GRU\", \"(Validation Set)\")\n",
    "plot_residuals(y_test, gru_test_pred, \"GRU\", \"(Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Baseline Models\n",
    "\n",
    "Load and compare with results from notebooks 06, 07, and 08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline results from notebooks 06, 07, and 08\n",
    "baseline_results = {\n",
    "    'Linear Regression': {'Test_R2': 0.208879, 'Test_RMSE_%': 19.756},\n",
    "    'Random Forest': {'Test_R2': 0.917825, 'Test_RMSE_%': 6.367},\n",
    "    'XGBoost': {'Test_R2': 0.904824, 'Test_RMSE_%': 6.852},\n",
    "    '1D CNN': {'Test_R2': 0.798880, 'Test_RMSE_%': 9.958},\n",
    "    'LSTM': {'Test_R2': 0.330670, 'Test_RMSE_%': 18.170}\n",
    "}\n",
    "\n",
    "# Add GRU results\n",
    "gru_results = {\n",
    "    'GRU': {\n",
    "        'Training_R2': gru_train_metrics['r2'],\n",
    "        'Training_RMSE_%': gru_train_metrics['rmse_percent'],\n",
    "        'Validation_R2': gru_val_metrics['r2'],\n",
    "        'Validation_RMSE_%': gru_val_metrics['rmse_percent'],\n",
    "        'Test_R2': gru_test_metrics['r2'],\n",
    "        'Test_RMSE_%': gru_test_metrics['rmse_percent']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nUsing {n_features} features with {sequence_length} timesteps\\n\")\n",
    "print(f\"{'Model':<20} {'Train R²':>10} {'Val R²':>10} {'Test R²':>10} {'Test RMSE (%)':>15}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "# Print baseline results\n",
    "for model_name, metrics in baseline_results.items():\n",
    "    print(f\"{model_name:<20} {'-':>10} {'-':>10} {metrics['Test_R2']:>10.6f} {metrics['Test_RMSE_%']:>15.3f}\")\n",
    "\n",
    "# Print GRU results\n",
    "for model_name, metrics in gru_results.items():\n",
    "    print(f\"{model_name:<20} {metrics['Training_R2']:>10.6f} {metrics['Validation_R2']:>10.6f} \"\n",
    "          f\"{metrics['Test_R2']:>10.6f} {metrics['Test_RMSE_%']:>15.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# R² comparison\n",
    "models = list(baseline_results.keys()) + list(gru_results.keys())\n",
    "test_r2 = [baseline_results[m]['Test_R2'] for m in baseline_results.keys()] + \\\n",
    "          [gru_results[m]['Test_R2'] for m in gru_results.keys()]\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple', 'brown']\n",
    "\n",
    "axes[0].bar(models, test_r2, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('Test Set R² Score Comparison (Higher is Better)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "for i, (model, score) in enumerate(zip(models, test_r2)):\n",
    "    axes[0].text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# RMSE comparison\n",
    "test_rmse = [baseline_results[m]['Test_RMSE_%'] for m in baseline_results.keys()] + \\\n",
    "            [gru_results[m]['Test_RMSE_%'] for m in gru_results.keys()]\n",
    "\n",
    "axes[1].bar(models, test_rmse, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('RMSE (% SOC)')\n",
    "axes[1].set_title('Test Set RMSE Comparison (Lower is Better)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "for i, (model, rmse) in enumerate(zip(models, test_rmse)):\n",
    "    axes[1].text(i, rmse + 0.2, f'{rmse:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GRU MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDataset Configuration:\")\n",
    "print(f\"   Total features available: {X_train_full.shape[2]}\")\n",
    "print(f\"   Features used: {n_features}\")\n",
    "print(f\"   Sequence length: {sequence_length} timesteps\")\n",
    "print(f\"   Training sequences: {X_train.shape[0]:,}\")\n",
    "print(f\"   Validation sequences: {X_val.shape[0]:,}\")\n",
    "print(f\"   Test sequences: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   GRU layers: 2 (48, 24 units)\")\n",
    "print(f\"   Dense layers: 2 (24, 1 units)\")\n",
    "print(f\"   Dropout rate: 0.3\")\n",
    "print(f\"   Recurrent dropout: 0.15\")\n",
    "\n",
    "print(\"\\nTraining Results:\")\n",
    "print(f\"   Epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"   Training time: {training_time/60:.2f} minutes\")\n",
    "print(f\"   Best validation loss: {np.min(history.history['val_loss']):.6f}\")\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"   Training R²: {gru_train_metrics['r2']:.6f}\")\n",
    "print(f\"   Validation R²: {gru_val_metrics['r2']:.6f}\")\n",
    "print(f\"   Test R²: {gru_test_metrics['r2']:.6f}\")\n",
    "print(f\"   Test RMSE: {gru_test_metrics['rmse_percent']:.3f}% SOC\")\n",
    "print(f\"   Test MAE: {gru_test_metrics['mae_percent']:.3f}% SOC\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_val_gap = gru_train_metrics['r2'] - gru_val_metrics['r2']\n",
    "status = \"✅ Good\" if train_val_gap < 0.05 else \"⚠️ Mild\" if train_val_gap < 0.1 else \"❌ Severe\"\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(f\"   Train-Val R² gap: {train_val_gap:.4f} ({status})\")\n",
    "print(f\"   Train-Test R² gap: {gru_train_metrics['r2'] - gru_test_metrics['r2']:.4f}\")\n",
    "\n",
    "# Compare with best baseline\n",
    "best_baseline = 'Random Forest'\n",
    "best_baseline_r2 = baseline_results[best_baseline]['Test_R2']\n",
    "best_baseline_rmse = baseline_results[best_baseline]['Test_RMSE_%']\n",
    "\n",
    "print(f\"\\nComparison with Best Baseline ({best_baseline}):\")\n",
    "print(f\"   Baseline Test R²: {best_baseline_r2:.6f}\")\n",
    "print(f\"   GRU Test R²: {gru_test_metrics['r2']:.6f}\")\n",
    "print(f\"   R² Difference: {gru_test_metrics['r2'] - best_baseline_r2:+.6f}\")\n",
    "print(f\"   Baseline Test RMSE: {best_baseline_rmse:.3f}% SOC\")\n",
    "print(f\"   GRU Test RMSE: {gru_test_metrics['rmse_percent']:.3f}% SOC\")\n",
    "print(f\"   RMSE Difference: {gru_test_metrics['rmse_percent'] - best_baseline_rmse:+.3f}% SOC\")\n",
    "\n",
    "# Compare with other RNNs\n",
    "lstm_r2 = baseline_results['LSTM']['Test_R2']\n",
    "lstm_rmse = baseline_results['LSTM']['Test_RMSE_%']\n",
    "\n",
    "print(f\"\\nComparison with LSTM:\")\n",
    "print(f\"   LSTM Test R²: {lstm_r2:.6f}\")\n",
    "print(f\"   GRU Test R²: {gru_test_metrics['r2']:.6f}\")\n",
    "print(f\"   R² Difference: {gru_test_metrics['r2'] - lstm_r2:+.6f}\")\n",
    "print(f\"   LSTM Test RMSE: {lstm_rmse:.3f}% SOC\")\n",
    "print(f\"   GRU Test RMSE: {gru_test_metrics['rmse_percent']:.3f}% SOC\")\n",
    "print(f\"   RMSE Difference: {gru_test_metrics['rmse_percent'] - lstm_rmse:+.3f}% SOC\")\n",
    "\n",
    "if gru_test_metrics['r2'] > best_baseline_r2:\n",
    "    print(f\"\\n✅ GRU outperforms all baseline models!\")\n",
    "elif gru_test_metrics['r2'] > lstm_r2:\n",
    "    print(f\"\\n✅ GRU outperforms LSTM for sequential modeling.\")\n",
    "else:\n",
    "    print(f\"\\nGRU performance is comparable to baseline models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = OUTPUT_DIR / 'final_gru_model.keras'\n",
    "gru_model.save(model_path)\n",
    "print(f\"✅ Model saved to {model_path}\")\n",
    "\n",
    "# Save results and metrics\n",
    "results = {\n",
    "    'model_type': 'GRU',\n",
    "    'sequence_length': sequence_length,\n",
    "    'n_features': n_features,\n",
    "    'feature_names': available_realistic,\n",
    "    'total_parameters': total_params,\n",
    "    'training_time': training_time,\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'metrics': {\n",
    "        'train': gru_train_metrics,\n",
    "        'val': gru_val_metrics,\n",
    "        'test': gru_test_metrics\n",
    "    },\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "results_path = OUTPUT_DIR / 'gru_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"✅ Results saved to {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}