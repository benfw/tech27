{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for SOC Estimation\n",
    "## Stanford TECH 27 Final Project - Notebook 03\n",
    "\n",
    "This notebook focuses on feature engineering for conventional machine learning models in SOC estimation.\n",
    "It builds upon the cleaned discharge data from `02_data_cleaning.ipynb`.\n",
    "\n",
    "### Goals:\n",
    "1. Load cleaned discharge data from previous notebook\n",
    "2. Engineer comprehensive features from raw battery measurements\n",
    "3. Create rolling statistics, lag features, and derived measurements\n",
    "4. Analyze feature importance and correlations\n",
    "5. Prepare engineered features for machine learning models\n",
    "\n",
    "### Output:\n",
    "- Feature engineering pipeline\n",
    "- Comprehensive engineered features dataset\n",
    "- Feature importance analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Import our custom utilities\n",
    "from data_processing_utils import (\n",
    "    load_cleaned_data, engineer_features, OUTPUT_DIR\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned discharge data from 02_data_cleaning.ipynb\n",
    "print(\"Loading cleaned discharge data...\")\n",
    "cleaned_data = load_cleaned_data()\n",
    "\n",
    "print(f\"\\nCleaned data overview:\")\n",
    "print(f\"Shape: {cleaned_data.shape}\")\n",
    "print(f\"Columns: {list(cleaned_data.columns)}\")\n",
    "print(f\"SOC range: {cleaned_data['SOC'].min():.3f} - {cleaned_data['SOC'].max():.3f}\")\n",
    "print(f\"Unique batteries: {cleaned_data['battery_id'].nunique()}\")\n",
    "print(f\"Unique files: {cleaned_data['filename'].nunique()}\")\n",
    "\n",
    "# Show sample of the data\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data for Feature Engineering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample for detailed feature engineering analysis\n",
    "# Use a single battery's data to demonstrate the pipeline\n",
    "\n",
    "sample_battery = cleaned_data['battery_id'].iloc[0]\n",
    "sample_data = cleaned_data[cleaned_data['battery_id'] == sample_battery].copy()\n",
    "\n",
    "print(f\"Sample data from battery: {sample_battery}\")\n",
    "print(f\"Sample shape: {sample_data.shape}\")\n",
    "print(f\"SOC range in sample: {sample_data['SOC'].min():.3f} - {sample_data['SOC'].max():.3f}\")\n",
    "print(f\"Duration: {(sample_data['Time'].max() - sample_data['Time'].min()) / 3600:.2f} hours\")\n",
    "\n",
    "# Visualize sample data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f'Sample Battery Data: {sample_battery}', fontsize=16)\n",
    "\n",
    "# SOC vs Time\n",
    "axes[0, 0].plot(sample_data['Time'], sample_data['SOC'], 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('SOC vs Time')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('State of Charge')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Voltage vs Time\n",
    "axes[0, 1].plot(sample_data['Time'], sample_data['Voltage_measured'], 'r-', linewidth=1)\n",
    "axes[0, 1].set_title('Voltage vs Time')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Voltage (V)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Current vs Time\n",
    "axes[1, 0].plot(sample_data['Time'], sample_data['Current_measured'], 'g-', linewidth=1)\n",
    "axes[1, 0].set_title('Current vs Time')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_ylabel('Current (A)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Voltage vs SOC\n",
    "axes[1, 1].scatter(sample_data['SOC'], sample_data['Voltage_measured'], alpha=0.6, s=10)\n",
    "axes[1, 1].set_title('Voltage vs SOC')\n",
    "axes[1, 1].set_xlabel('State of Charge')\n",
    "axes[1, 1].set_ylabel('Voltage (V)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to sample data\n",
    "print(\"Testing feature engineering pipeline on sample data...\")\n",
    "sample_features = engineer_features(sample_data)\n",
    "\n",
    "print(f\"\\nFeature Engineering Results:\")\n",
    "print(f\"Original data shape: {sample_data.shape}\")\n",
    "print(f\"Engineered features shape: {sample_features.shape}\")\n",
    "\n",
    "# Identify feature types\n",
    "metadata_cols = ['SOC', 'battery_id', 'filename', 'test_id', 'ambient_temperature']\n",
    "feature_cols = [col for col in sample_features.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Target column: SOC\")\n",
    "print(f\"Metadata columns: {[col for col in metadata_cols if col in sample_features.columns]}\")\n",
    "\n",
    "# Categorize features\n",
    "basic_features = [col for col in feature_cols if col in ['voltage', 'current', 'temperature', 'time']]\n",
    "derived_features = [col for col in feature_cols if any(x in col for x in ['power', 'abs_current', 'ratio', 'energy'])]\n",
    "change_features = [col for col in feature_cols if '_change' in col]\n",
    "rolling_features = [col for col in feature_cols if 'rolling' in col]\n",
    "cumulative_features = [col for col in feature_cols if 'cumulative' in col]\n",
    "time_features = [col for col in feature_cols if any(x in col for x in ['time_since', 'time_normalized'])]\n",
    "lag_features = [col for col in feature_cols if '_lag_' in col]\n",
    "statistical_features = [col for col in feature_cols if any(x in col for x in ['vs_mean', 'percentile'])]\n",
    "\n",
    "print(f\"\\nFeature Categories:\")\n",
    "print(f\"  Basic measurements: {len(basic_features)} - {basic_features[:3]}...\")\n",
    "print(f\"  Derived features: {len(derived_features)} - {derived_features[:3]}...\")\n",
    "print(f\"  Rate of change: {len(change_features)} - {change_features[:3]}...\")\n",
    "print(f\"  Rolling statistics: {len(rolling_features)} - {rolling_features[:3]}...\")\n",
    "print(f\"  Cumulative features: {len(cumulative_features)} - {cumulative_features}\")\n",
    "print(f\"  Time-based features: {len(time_features)} - {time_features}\")\n",
    "print(f\"  Lag features: {len(lag_features)} - {lag_features[:3]}...\")\n",
    "print(f\"  Statistical features: {len(statistical_features)} - {statistical_features[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature statistics and data quality\n",
    "print(\"Analyzing feature quality...\")\n",
    "\n",
    "# Check for data quality issues\n",
    "nan_counts = sample_features[feature_cols].isnull().sum()\n",
    "inf_counts = np.isinf(sample_features[feature_cols]).sum()\n",
    "zero_variance = sample_features[feature_cols].var() == 0\n",
    "\n",
    "print(f\"\\nData Quality Checks:\")\n",
    "print(f\"Features with NaN values: {(nan_counts > 0).sum()}\")\n",
    "print(f\"Features with Inf values: {(inf_counts > 0).sum()}\")\n",
    "print(f\"Features with zero variance: {zero_variance.sum()}\")\n",
    "\n",
    "if (nan_counts > 0).sum() > 0:\n",
    "    print(f\"\\nFeatures with NaN:\")\n",
    "    print(nan_counts[nan_counts > 0].head())\n",
    "\n",
    "if (inf_counts > 0).sum() > 0:\n",
    "    print(f\"\\nFeatures with Inf:\")\n",
    "    print(inf_counts[inf_counts > 0].head())\n",
    "\n",
    "if zero_variance.sum() > 0:\n",
    "    print(f\"\\nFeatures with zero variance:\")\n",
    "    print(zero_variance[zero_variance].index.tolist())\n",
    "\n",
    "# Show feature statistics for first 10 features\n",
    "print(f\"\\nFeature Statistics (first 10 features):\")\n",
    "feature_stats = sample_features[feature_cols[:10]].describe()\n",
    "print(feature_stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features and their relationships with SOC\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Select key features to visualize\n",
    "key_features = [\n",
    "    'voltage', 'current', 'temperature', 'power', 'cumulative_discharge',\n",
    "    'voltage_rolling_mean_10', 'voltage_change', 'voltage_current_ratio', 'time_normalized'\n",
    "]\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    if i < 9 and feature in sample_features.columns:\n",
    "        # Scatter plot of feature vs SOC\n",
    "        axes[i].scatter(sample_features[feature], sample_features['SOC'] * 100, alpha=0.6, s=1)\n",
    "        axes[i].set_xlabel(feature.replace('_', ' ').title())\n",
    "        axes[i].set_ylabel('SOC (%)')\n",
    "        axes[i].set_title(f'{feature.replace(\"_\", \" \").title()} vs SOC')\n",
    "        axes[i].grid(alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        corr = sample_features[feature].corr(sample_features['SOC'])\n",
    "        axes[i].text(0.05, 0.95, f'r = {corr:.3f}', transform=axes[i].transAxes, \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    elif feature not in sample_features.columns:\n",
    "        axes[i].text(0.5, 0.5, f'{feature}\\nnot found', ha='center', va='center', \n",
    "                    transform=axes[i].transAxes)\n",
    "        axes[i].set_title(f'{feature} (Missing)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature correlation heatmap (for a subset of features)\n",
    "correlation_features = key_features[:6]  # Top 6 for readability\n",
    "available_features = [f for f in correlation_features if f in sample_features.columns]\n",
    "\n",
    "if len(available_features) > 1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    corr_matrix = sample_features[available_features + ['SOC']].corr()\n",
    "    \n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "    ax.set_title('Feature Correlation Matrix (Key Features + SOC)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance using mutual information and random forest\n",
    "if len(sample_features) > 100:\n",
    "    print(\"Calculating feature importance...\")\n",
    "    \n",
    "    # Prepare data for importance analysis\n",
    "    X = sample_features[feature_cols].copy()\n",
    "    y = sample_features['SOC']\n",
    "    \n",
    "    # Handle any remaining NaN/inf values\n",
    "    X_clean = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    # Mutual information\n",
    "    print(\"Computing mutual information scores...\")\n",
    "    mi_scores = mutual_info_regression(X_clean, y, random_state=42)\n",
    "    mi_importance = pd.Series(mi_scores, index=feature_cols).sort_values(ascending=False)\n",
    "    \n",
    "    # Random forest feature importance\n",
    "    print(\"Training Random Forest for feature importance...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_clean, y)\n",
    "    rf_importance = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 15 Features by Mutual Information:\")\n",
    "    print(mi_importance.head(15).round(4))\n",
    "    \n",
    "    print(f\"\\nTop 15 Features by Random Forest Importance:\")\n",
    "    print(rf_importance.head(15).round(4))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Mutual Information\n",
    "    mi_importance.head(15).plot(kind='barh', ax=axes[0])\n",
    "    axes[0].set_title('Top 15 Features - Mutual Information')\n",
    "    axes[0].set_xlabel('Mutual Information Score')\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_importance.head(15).plot(kind='barh', ax=axes[1])\n",
    "    axes[1].set_title('Top 15 Features - Random Forest Importance')\n",
    "    axes[1].set_xlabel('Feature Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save feature importance for later use\n",
    "    feature_importance = {\n",
    "        'mutual_info': mi_importance.to_dict(),\n",
    "        'random_forest': rf_importance.to_dict(),\n",
    "        'top_features_mi': mi_importance.head(15).index.tolist(),\n",
    "        'top_features_rf': rf_importance.head(15).index.tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nFeature importance analysis complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient data for feature importance analysis\")\n",
    "    feature_importance = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Dataset Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to the full cleaned dataset\n",
    "print(\"Applying feature engineering to full dataset...\")\n",
    "print(\"Note: This may take several minutes for large datasets\")\n",
    "\n",
    "# Process data by battery to manage memory and show progress\n",
    "unique_batteries = cleaned_data['battery_id'].unique()\n",
    "engineered_data_list = []\n",
    "\n",
    "print(f\"Processing {len(unique_batteries)} unique batteries...\")\n",
    "\n",
    "for battery_id in tqdm(unique_batteries, desc=\"Engineering features\"):\n",
    "    # Get data for this battery\n",
    "    battery_data = cleaned_data[cleaned_data['battery_id'] == battery_id].copy()\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    try:\n",
    "        engineered_battery_data = engineer_features(battery_data)\n",
    "        engineered_data_list.append(engineered_battery_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing battery {battery_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Combine all engineered data\n",
    "if engineered_data_list:\n",
    "    full_engineered_data = pd.concat(engineered_data_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nFull dataset feature engineering complete!\")\n",
    "    print(f\"Final shape: {full_engineered_data.shape}\")\n",
    "    print(f\"Number of features: {len([col for col in full_engineered_data.columns if col not in metadata_cols])}\")\n",
    "    print(f\"Memory usage: {full_engineered_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Data quality check on full dataset\n",
    "    feature_cols_full = [col for col in full_engineered_data.columns if col not in metadata_cols]\n",
    "    nan_count = full_engineered_data[feature_cols_full].isnull().sum().sum()\n",
    "    inf_count = np.isinf(full_engineered_data[feature_cols_full]).sum().sum()\n",
    "    \n",
    "    print(f\"\\nData quality check:\")\n",
    "    print(f\"Total NaN values: {nan_count}\")\n",
    "    print(f\"Total Inf values: {inf_count}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data was successfully processed!\")\n",
    "    full_engineered_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save engineered features and analysis results\n",
    "if full_engineered_data is not None:\n",
    "    \n",
    "    # Save full engineered dataset\n",
    "    engineered_data_path = OUTPUT_DIR / 'engineered_features.csv'\n",
    "    full_engineered_data.to_csv(engineered_data_path, index=False)\n",
    "    print(f\"Saved full engineered dataset to {engineered_data_path}\")\n",
    "    print(f\"File size: {engineered_data_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Save sample engineered features for quick access\n",
    "    sample_features_path = OUTPUT_DIR / 'sample_engineered_features.pkl'\n",
    "    with open(sample_features_path, 'wb') as f:\n",
    "        pickle.dump(sample_features, f)\n",
    "    print(f\"Saved sample engineered features to {sample_features_path}\")\n",
    "    \n",
    "    # Save feature importance if calculated\n",
    "    if 'feature_importance' in locals() and feature_importance is not None:\n",
    "        importance_path = OUTPUT_DIR / 'feature_importance.pkl'\n",
    "        with open(importance_path, 'wb') as f:\n",
    "            pickle.dump(feature_importance, f)\n",
    "        print(f\"Saved feature importance analysis to {importance_path}\")\n",
    "    \n",
    "    # Save feature engineering metadata\n",
    "    feature_metadata = {\n",
    "        'total_features': len(feature_cols_full) if 'feature_cols_full' in locals() else len(feature_cols),\n",
    "        'feature_categories': {\n",
    "            'basic_measurements': len(basic_features),\n",
    "            'derived_features': len(derived_features),\n",
    "            'change_features': len(change_features),\n",
    "            'rolling_features': len(rolling_features),\n",
    "            'cumulative_features': len(cumulative_features),\n",
    "            'time_features': len(time_features),\n",
    "            'lag_features': len(lag_features),\n",
    "            'statistical_features': len(statistical_features)\n",
    "        },\n",
    "        'data_shape': full_engineered_data.shape,\n",
    "        'unique_batteries': full_engineered_data['battery_id'].nunique(),\n",
    "        'soc_range': (full_engineered_data['SOC'].min(), full_engineered_data['SOC'].max()),\n",
    "        'data_quality': {\n",
    "            'nan_values': nan_count if 'nan_count' in locals() else 0,\n",
    "            'inf_values': inf_count if 'inf_count' in locals() else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = OUTPUT_DIR / 'feature_engineering_metadata.pkl'\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(feature_metadata, f)\n",
    "    print(f\"Saved feature engineering metadata to {metadata_path}\")\n",
    "    \n",
    "    print(f\"\\n✅ Feature engineering complete!\")\n",
    "    print(f\"Ready for sequential data preparation (Notebook 04)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No engineered features to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "### ✅ Completed Tasks:\n",
    "1. **Data Loading**: Loaded cleaned discharge data from `02_data_cleaning.ipynb`\n",
    "2. **Feature Engineering**: Created comprehensive feature set with 40+ engineered features\n",
    "3. **Feature Categories**: Implemented multiple feature types:\n",
    "   - Basic measurements (voltage, current, temperature, time)\n",
    "   - Derived features (power, ratios, absolute values)\n",
    "   - Rate of change features (derivatives)\n",
    "   - Rolling statistics (multiple window sizes)\n",
    "   - Cumulative features (discharge capacity, energy)\n",
    "   - Time-based features (normalized time, time since start)\n",
    "   - Lag features (historical values)\n",
    "   - Statistical features (deviations, percentiles)\n",
    "4. **Feature Analysis**: Conducted correlation and importance analysis\n",
    "5. **Quality Control**: Comprehensive data quality checks and validation\n",
    "\n",
    "### 📊 Key Results:\n",
    "- **Feature Count**: 40+ engineered features from basic measurements\n",
    "- **Data Quality**: Proper handling of NaN/Inf values and data validation\n",
    "- **Feature Importance**: Identified most predictive features for SOC estimation\n",
    "- **Scalable Pipeline**: Efficient processing for large datasets\n",
    "\n",
    "### 📁 Output Files:\n",
    "- `processed_data/engineered_features.csv`: Full dataset with all engineered features\n",
    "- `processed_data/sample_engineered_features.pkl`: Sample dataset for quick analysis\n",
    "- `processed_data/feature_importance.pkl`: Feature importance analysis results\n",
    "- `processed_data/feature_engineering_metadata.pkl`: Pipeline metadata and statistics\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. **Notebook 04**: Sequential Data Preparation for Deep Learning Models\n",
    "2. **Notebook 05**: Final Data Processing and Train/Val/Test Splits\n",
    "\n",
    "The engineered features are now ready for both conventional ML models and as inputs to sequential models for deep learning approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}